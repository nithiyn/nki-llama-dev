#!/bin/bash
# nki-llama.config - Shared configuration for NKI-LLAMA projects

# Project Structure
export NKI_ROOT="${HOME}/nki-llama"
export NKI_SRC="${NKI_ROOT}/src"
export NKI_FINETUNE="${NKI_SRC}/fine-tune"
export NKI_INFERENCE="${NKI_SRC}/inference"
export NKI_FINETUNE_SCRIPTS="${NKI_FINETUNE}/scripts"
export NKI_INFERENCE_SCRIPTS="${NKI_INFERENCE}/scripts"
export NKI_LOGS="${NKI_ROOT}/logs"
export NKI_MODELS="${HOME}/models"
export NKI_COMPILED="${HOME}/traced_model"

# Model Configuration
export MODEL_ID="${MODEL_ID:-meta-llama/Meta-Llama-3-8B}"
export MODEL_NAME="${MODEL_NAME:-llama-3-8b}"
export HF_TOKEN="${HF_TOKEN:-}"

# Training Configuration
export BATCH_SIZE="${BATCH_SIZE:-1}"
export MAX_STEPS="${MAX_STEPS:-1000}"
export SEQ_LENGTH="${SEQ_LENGTH:-2048}"
export TENSOR_PARALLEL_SIZE="${TENSOR_PARALLEL_SIZE:-8}"
export LEARNING_RATE="${LEARNING_RATE:-5e-5}"

# Inference Configuration
export INFERENCE_PORT="${INFERENCE_PORT:-8080}"
export MAX_MODEL_LEN="${MAX_MODEL_LEN:-2048}"
export MAX_NUM_SEQS="${MAX_NUM_SEQS:-4}"
export ENABLE_NKI="${ENABLE_NKI:-true}"

# Neuron Configuration
export NEURON_VENV="/opt/aws_neuronx_venv_pytorch_2_6"
export NEURON_INFERENCE_VENV="/opt/aws_neuronx_venv_pytorch_2_6_nxd_inference"
export NEURON_COMPILE_CACHE="${HOME}/.cache/neuron"
export NEURON_RT_NUM_CORES="${NEURON_RT_NUM_CORES:-8}"

# vLLM Configuration
export VLLM_REPO="${HOME}/upstreaming-to-vllm"
export VLLM_BRANCH="neuron-2.22-vllm-v0.7.2"
export VLLM_NEURON_FRAMEWORK="neuronx-distributed-inference"

# Dataset Configuration
export DATASET_NAME="${DATASET_NAME:-databricks/databricks-dolly-15k}"
export DATASET_DIR="${NKI_FINETUNE}/datasets"
export TOKENIZER_DIR="${NKI_FINETUNE}/model_assets/llama3_tokenizer"

# Checkpoint Paths
export HF_WEIGHTS_DIR="${NKI_FINETUNE}/model_assets/llama3-8B_hf_weights_bin"
export PRETRAINED_CKPT="${NKI_FINETUNE}/model_assets/pckpt"
export NEMO_EXPERIMENTS="${NKI_FINETUNE}/nemo_experiments"

# Jupyter Configuration
export JUPYTER_PORT="${JUPYTER_PORT:-8888}"
export JUPYTER_VENV="${NKI_ROOT}/venv"

# Function to print configuration
print_config() {
    echo "NKI-LLAMA Configuration:"
    echo "======================="
    echo "Project Root: ${NKI_ROOT}"
    echo "Model: ${MODEL_NAME} (${MODEL_ID})"
    echo "Fine-tune Scripts: ${NKI_FINETUNE_SCRIPTS}"
    echo "Inference Scripts: ${NKI_INFERENCE_SCRIPTS}"
    echo "Tensor Parallel Size: ${TENSOR_PARALLEL_SIZE}"
    echo "Sequence Length: ${SEQ_LENGTH}"
}
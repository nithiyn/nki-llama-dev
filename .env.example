# Example environment file for NKI-LLAMA
# Copy this to .env and update with your values

# Hugging Face Configuration
HF_TOKEN=your_huggingface_token_here
MODEL_ID=meta-llama/Meta-Llama-3-8B
MODEL_NAME=llama-3-8b

# Training Configuration
BATCH_SIZE=1
MAX_STEPS=1000
SEQ_LENGTH=2048
LEARNING_RATE=5e-5

# Inference Configuration
INFERENCE_PORT=8080
MAX_MODEL_LEN=2048
MAX_NUM_SEQS=4
TENSOR_PARALLEL_SIZE=8

# Dataset Configuration
DATASET_NAME=databricks/databricks-dolly-15k

# Neuron Configuration
NEURON_RT_NUM_CORES=8

# Jupyter Configuration
JUPYTER_PORT=8888